% METHODS USED
@inproceedings{BiVAECF,
  title     = {Bilateral Variational Autoencoder for Collaborative Filtering},
  author    = {Truong, Quoc-Tuan and Salah, Aghiles and Lauw, Hady W},
  booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  pages     = {292--300},
  year      = {2021}
}

@article{ItemKNN,
  title     = {Item-based top-n recommendation algorithms},
  author    = {Deshpande, Mukund and Karypis, George},
  journal   = {ACM Transactions on Information Systems (TOIS)},
  volume    = {22},
  number    = {1},
  pages     = {143--177},
  year      = {2004},
  publisher = {ACM New York, NY, USA}
}

@inproceedings{BPR,
  title     = {Bayesian personalized ranking from implicit feedback},
  author    = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars BPR},
  booktitle = {Proc. of Uncertainty in Artificial Intelligence},
  pages     = {452--461},
  year      = {2014}
}

@article{koren_matrix_2009,
  title    = {Matrix {Factorization} {Techniques} for {Recommender} {Systems}},
  volume   = {42},
  issn     = {0018-9162},
  url      = {http://ieeexplore.ieee.org/document/5197422/},
  doi      = {10.1109/MC.2009.263},
  language = {en},
  number   = {8},
  urldate  = {2021-05-10},
  journal  = {Computer},
  author   = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  month    = aug,
  year     = {2009},
  pages    = {30--37},
  file     = {Koren et al. - 2009 - Matrix Factorization Techniques for Recommender Sy.pdf:/Users/wsa/Zotero/storage/EKEFQXRZ/Koren et al. - 2009 - Matrix Factorization Techniques for Recommender Sy.pdf:application/pdf}
}

@article{lemhadri_lassonet_2021,
  title      = {{LassoNet}: {A} {Neural} {Network} with {Feature} {Sparsity}},
  shorttitle = {{LassoNet}},
  url        = {http://arxiv.org/abs/1907.12207},
  abstract   = {Much work has been done recently to make neural networks more interpretable, and one approach is to arrange for the network to use only a subset of the available features. In linear models, Lasso (or 1-regularized) regression assigns zero weights to the most irrelevant or redundant features, and is widely used in data science. However the Lasso only applies to linear models. Here we introduce LassoNet, a neural network framework with global feature selection. Our approach achieves feature sparsity by allowing a feature to participate in a hidden unit only if its linear representative is active. Unlike other approaches to feature selection for neural nets, our method uses a modiﬁed objective function with constraints, and so integrates feature selection with the parameter learning directly. As a result, it delivers an entire regularization path of solutions with a range of feature sparsity. In experiments with real and simulated data, LassoNet signiﬁcantly outperforms state-ofthe-art methods for feature selection and regression. The LassoNet method uses projected proximal gradient descent, and generalizes directly to deep networks. It can be implemented by adding just a few lines of code to a standard neural network.},
  language   = {en},
  urldate    = {2021-05-10},
  journal    = {arXiv:1907.12207 [cs, stat]},
  author     = {Lemhadri, Ismael and Ruan, Feng and Abraham, Louis and Tibshirani, Robert},
  month      = feb,
  year       = {2021},
  note       = {arXiv: 1907.12207},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  annote     = {Comment: 18 pages, 10 fg. arXiv admin note: text overlap with arXiv:1901.09346 by other authors},
  file       = {Lemhadri et al. - 2021 - LassoNet A Neural Network with Feature Sparsity.pdf:/Users/wsa/Zotero/storage/RI8DGYRG/Lemhadri et al. - 2021 - LassoNet A Neural Network with Feature Sparsity.pdf:application/pdf}
}

% cornac
@article{cornac,
  title   = {Cornac: A Comparative Framework for Multimodal Recommender Systems},
  author  = {Salah, Aghiles and Truong, Quoc-Tuan and Lauw, Hady W},
  journal = {Journal of Machine Learning Research},
  volume  = {21},
  number  = {95},
  pages   = {1--5},
  year    = {2020}
}

% amazon data
@article{amazon_data,
  title      = {Power of the {Few}: {Analyzing} the {Impact} of {Influential} {Users} in {Collaborative} {Recommender} {Systems}},
  shorttitle = {Power of the {Few}},
  url        = {http://arxiv.org/abs/1905.08031},
  doi        = {10.1145/3320435.3320464},
  abstract   = {Like other social systems, in collaborative filtering a small number of “influential” users may have a large impact on the recommendations of other users, thus affecting the overall behavior of the system. Identifying influential users and studying their impact on other users is an important problem because it provides insight into how small groups can inadvertently or intentionally affect the behavior of the system as a whole. Modeling these influences can also shed light on patterns and relationships that would otherwise be difficult to discern, hopefully leading to more transparency in how the system generates personalized content. In this work we first formalize the notion of “influence” in collaborative filtering using an Influence Discrimination Model. We then empirically identify and characterize influential users and analyze their impact on the system under different underlying recommendation algorithms and across three different recommendation domains: job, movie and book recommendations. Insights from these experiments can help in designing systems that are not only optimized for accuracy, but are also tuned to mitigate the impact of influential users when it might lead to potential imbalance or unfairness in the system’s outcomes.},
  language   = {en},
  urldate    = {2021-06-02},
  journal    = {Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization},
  author     = {Eskandanian, Farzad and Sonboli, Nasim and Mobasher, Bamshad},
  month      = jun,
  year       = {2019},
  note       = {arXiv: 1905.08031},
  keywords   = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
  pages      = {225--233},
  file       = {Eskandanian et al. - 2019 - Power of the Few Analyzing the Impact of Influent.pdf:/Users/wsa/Zotero/storage/SGPBJQUL/Eskandanian et al. - 2019 - Power of the Few Analyzing the Impact of Influent.pdf:application/pdf}
}

% from zotero

@article{candes_power_2009,
  title      = {The {Power} of {Convex} {Relaxation}: {Near}-{Optimal} {Matrix} {Completion}},
  shorttitle = {The {Power} of {Convex} {Relaxation}},
  url        = {http://arxiv.org/abs/0903.1476},
  abstract   = {This paper is concerned with the problem of recovering an unknown matrix from a small fraction of its entries. This is known as the matrix completion problem, and comes up in a great number of applications, including the famous Netﬂix Prize and other similar questions in collaborative ﬁltering. In general, accurate recovery of a matrix from a small number of entries is impossible; but the knowledge that the unknown matrix has low rank radically changes this premise, making the search for solutions meaningful.},
  language   = {en},
  urldate    = {2021-05-10},
  journal    = {arXiv:0903.1476 [cs, math]},
  author     = {Candes, Emmanuel J. and Tao, Terence},
  month      = mar,
  year       = {2009},
  note       = {arXiv: 0903.1476},
  keywords   = {Computer Science - Information Theory},
  annote     = {Comment: 51 pages, 12 figures},
  file       = {Candes and Tao - 2009 - The Power of Convex Relaxation Near-Optimal Matri.pdf:/Users/wsa/Zotero/storage/FH74QJEE/Candes and Tao - 2009 - The Power of Convex Relaxation Near-Optimal Matri.pdf:application/pdf}
}

@article{goldberg_using_1992,
  title    = {Using collaborative filtering to weave an information tapestry},
  volume   = {35},
  issn     = {0001-0782, 1557-7317},
  url      = {https://dl.acm.org/doi/10.1145/138859.138867},
  doi      = {10.1145/138859.138867},
  abstract = {Tapestry is an experimental system that manages an incoming stream of electronic documents, including electronic mail, news wire stories and NetNews articles. In common with some recent mail systems, Tapestry uses filtering to cope with large volumes of incoming documents. Where Tapestry differs from these systems is in its philosophy that humans provide the most reliable evaluation of documents, and so it uses collaborative filtering, which it implements by having users annotate documents, and then allowing filtering using those annotations. Because annotations are not known at the time that documents arrive, Tapestry filters must not only test incoming documents, but must also run repeatedly over the entire database of documents. This paper is a report on the Tapestry design and the status of its implementation.},
  language = {en},
  number   = {12},
  urldate  = {2021-05-10},
  journal  = {Communications of the ACM},
  author   = {Goldberg, David and Nichols, David and Oki, Brian M. and Terry, Douglas},
  month    = dec,
  year     = {1992},
  pages    = {61--70},
  file     = {Goldberg et al. - 1992 - Using collaborative filtering to weave an informat.pdf:/Users/wsa/Zotero/storage/AB84LXSF/Goldberg et al. - 1992 - Using collaborative filtering to weave an informat.pdf:application/pdf}
}

@article{guo2014merging,
  title     = {Merging trust in collaborative filtering to alleviate data sparsity and cold start},
  author    = {Guo, Guibing and Zhang, Jie and Thalmann, Daniel},
  journal   = {Knowledge-Based Systems},
  volume    = {57},
  pages     = {57--68},
  year      = {2014},
  publisher = {Elsevier}
}

@inproceedings{said2012analyzing,
  title     = {Analyzing weighting schemes in collaborative filtering: cold start, post cold start and power users},
  author    = {Said, Alan and Jain, Brijnesh J and Albayrak, Sahin},
  booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
  pages     = {2035--2040},
  year      = {2012}
}

@inproceedings{sedhain2014social,
  title     = {Social collaborative filtering for cold-start recommendations},
  author    = {Sedhain, Suvash and Sanner, Scott and Braziunas, Darius and Xie, Lexing and Christensen, Jordan},
  booktitle = {Proceedings of the 8th ACM Conference on Recommender systems},
  pages     = {345--348},
  year      = {2014}
}

@inproceedings{chae2020ar,
  title     = {AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems},
  author    = {Chae, Dong-Kyu and Kim, Jihoo and Chau, Duen Horng and Kim, Sang-Wook},
  booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {1251--1260},
  year      = {2020}
}

@article{wei2017collaborative,
  title     = {Collaborative filtering and deep learning based recommendation system for cold start items},
  author    = {Wei, Jian and He, Jianhua and Chen, Kai and Zhou, Yi and Tang, Zuoyin},
  journal   = {Expert Systems with Applications},
  volume    = {69},
  pages     = {29--39},
  year      = {2017},
  publisher = {Elsevier}
}

@inproceedings{liang2018variational,
  title     = {Variational autoencoders for collaborative filtering},
  author    = {Liang, Dawen and Krishnan, Rahul G and Hoffman, Matthew D and Jebara, Tony},
  booktitle = {Proceedings of the 2018 world wide web conference},
  pages     = {689--698},
  year      = {2018}
}